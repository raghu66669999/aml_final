{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "vCRMTMn6B7W0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZseTs7Gb6oCt",
        "outputId": "1f3f5984-7dbb-422f-cf75-a423e4b788cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('filename.pickle', 'wb') as handle:\n",
        "    pickle.dump(wv, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "E14khCflh-pF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EEcJ4FS323g",
        "outputId": "3bcd0b5f-e2f1-4a1b-933a-d09c91b42ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 109/109 [39:34<00:00, 21.78s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "filtered_data=pd.read_csv('anomalous_reviews_df.csv')\n",
        "filtered_data= filtered_data[filtered_data['anomalous_reviews'] == True]\n",
        "def text_to_embeddings(text, model):\n",
        "    tokens = text.split()\n",
        "    embeddings = [model[token] for token in tokens if token in model]\n",
        "    return np.array(embeddings)\n",
        "def add_noise_to_embeddings(embeddings, noise_level=0.1):\n",
        "    noise = np.random.normal(0, noise_level, embeddings.shape)\n",
        "    return embeddings + noise\n",
        "def embeddings_to_text(embeddings, model):\n",
        "    words = [model.most_similar([embedding], topn=1)[0][0] for embedding in embeddings]\n",
        "    return ' '.join(words)\n",
        "def generate_glove(generated_txt,wv):\n",
        "  modified_text = []\n",
        "  for generated_text in tqdm(generated_txt):\n",
        "    # Transform text to embeddings\n",
        "    embeddings = text_to_embeddings(generated_text, wv)\n",
        "    # Add noise to create variations\n",
        "    noisy_embeddings = add_noise_to_embeddings(embeddings)\n",
        "    # Convert embeddings back to text\n",
        "    modified_text.append(embeddings_to_text(noisy_embeddings, wv))\n",
        "  return modified_text\n",
        "\n",
        "dfx=pd.DataFrame(generate_glove(filtered_data['GeneratedText'],wv))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_data[filtered_data['Anomaly_mp'] == True])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7eY3z3MCa9C",
        "outputId": "f1e099ac-af96-4a6d-f131-d0ec699b4274"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfx.to_csv('final_glovex.csv', index=False)"
      ],
      "metadata": {
        "id": "O2P4EHn-tNUd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfx[0][0]"
      ],
      "metadata": {
        "id": "-XL0nMZXKRVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data['GeneratedText'][2]"
      ],
      "metadata": {
        "id": "GAGXE3W-Kcp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "filtered_datat=pd.read_csv('final_amazon_reviews_with_gptreviews.csv')\n",
        "filtered_datat= filtered_datat['Text']\n",
        "def text_to_embeddings(text, model):\n",
        "    tokens = text.split()\n",
        "    embeddings = [model[token] for token in tokens if token in model]\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def add_noise_to_embeddings(embeddings, noise_level=0.1):\n",
        "    noise = np.random.normal(0, noise_level, embeddings.shape)\n",
        "    return embeddings + noise\n",
        "\n",
        "def embeddings_to_text(embeddings, model):\n",
        "    words = [model.most_similar([embedding], topn=1)[0][0] for embedding in embeddings]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def generate_glove(generated_txt,wv):\n",
        "  modified_text = []\n",
        "  for generated_text in tqdm(generated_txt):\n",
        "    # Transform text to embeddings\n",
        "    embeddings = text_to_embeddings(generated_text, wv)\n",
        "\n",
        "    # Add noise to create variations\n",
        "    noisy_embeddings = add_noise_to_embeddings(embeddings)\n",
        "\n",
        "    # Convert embeddings back to text\n",
        "    modified_text.append(embeddings_to_text(noisy_embeddings, wv))\n",
        "  return modified_text\n",
        "\n",
        "dfxt=pd.DataFrame(generate_glove(filtered_datat,wv))\n",
        "dfxt.to_csv('final_glove_text.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9H_Cgrx8nbx",
        "outputId": "544d432b-3815-4527-dac4-c60274c5446d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 534/534 [2:50:09<00:00, 19.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_datat.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9tUgGi88W9D",
        "outputId": "2cc99c8f-29ac-4fc3-e02f-0d248c919063"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I purchased as a giveaway for a baby shower wi...\n",
              "1    Great taste. Great ingredients. Great texture....\n",
              "2    Amazing concept... not fried, not even baked.....\n",
              "3    OVER THE YEARS I HAVE BOUGHT A LOT OF POPCORN ...\n",
              "4    Good, but I add a cup of whole wheat flour and...\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}